"""
æ•´åˆspaCyä¸æ˜“ç»å¦è±¡åˆ†æçš„ç³»ç»Ÿ
æ¶æ„ç‰ˆæœ¬ï¼š2.0
åŠŸèƒ½æ¨¡å—ï¼š
1. å¥å­çº§æƒ…æ„Ÿ-å¦è±¡æ˜ å°„
2. åŠ¨æ€å¦è±¡è§£é‡Šç³»ç»Ÿ
3. å¤šå±‚æ¬¡åˆ†ææŠ¥å‘Š
"""
import spacy
from pathlib import Path
from typing import List, Dict, Tuple
import argparse
import sys
import time
import math
from collections import defaultdict
from gua_config import GUAS, INTENSITY_RANK

# é…ç½®å‚æ•°
DEFAULT_MODEL = "zh_core_web_lg"

class YijingAnalyzer:
    """æ˜“ç»åˆ†æå¼•æ“ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    # æ·»åŠ æƒ…æ„Ÿè¯åº“ç¼“å­˜
    SENTIMENT_LEXICON = {
        # åŸºç¡€æƒ…æ„Ÿè¯
        "å¥½": 0.5, "ä¼˜ç§€": 0.7, "ç³Ÿç³•": -0.6,
        "å¿«ä¹": 0.6, "æ‚²ä¼¤": -0.5, "æ„¤æ€’": -0.7,
        "å–œæ‚¦": 0.8, "ç„¦è™‘": -0.4, "å¹³é™": 0.3,
        "æ»¡æ„": 0.6, "å¤±æœ›": -0.5, "æœŸå¾…": 0.4,
        # æ‰©å±•æƒ…æ„Ÿè¯
        "ç¾å¥½": 0.7, "å“è¶Š": 0.8, "æ°å‡º": 0.8,
        "æ¬¢æ¬£": 0.7, "æ„‰æ‚¦": 0.6, "èˆ’ç•…": 0.5,
        "å¿§ä¼¤": -0.6, "ç—›è‹¦": -0.8, "ææƒ§": -0.7,
        "çƒ¦æ¼": -0.5, "æ²®ä¸§": -0.6, "ç»æœ›": -0.9,
        "æ¸©æš–": 0.4, "æ¸©é¦¨": 0.5, "ç¥¥å’Œ": 0.6,
        "å†·æ¼ ": -0.4, "é˜´éƒ": -0.5, "å‹æŠ‘": -0.6
    }
    NEGATION_WORDS = {"ä¸", "æ²¡", "é", "æœª", "åˆ«", "è«", "å‹¿", "æ— ", "å¦", "ä¼‘", "ç»", "éš¾", "å†³", "å¿Œ"}

    def __init__(self):
        if not hasattr(self.__class__, '_nlp'):
            self.__class__._nlp = self._load_model()
        self.nlp = self.__class__._nlp
        self.sentences = []
        self.gua_results = []
        self.polarity_stats = defaultdict(int)
        self.intensity_stats = defaultdict(int)
        
    def _load_model(self) -> spacy.language.Language:
        try:
            nlp = spacy.load(DEFAULT_MODEL)
            if not nlp.has_pipe("sentiment"):
                nlp.add_pipe("sentiment", last=True)
            return nlp
        except OSError:
            print(f"è¯·å…ˆå®‰è£…ä¸­æ–‡æ¨¡å‹: python -m spacy download {DEFAULT_MODEL}")
            sys.exit(1)
    
    def analyze_text(self, text: str) -> None:
        doc = self.nlp(text)
        self.sentences = [sent.text for sent in doc.sents]
        self._analyze_sentiments(doc)
        self._analyze_polarity_and_intensity()
    
    def _analyze_sentiments(self, doc) -> None:
        for sent in doc.sents:
            sentiment = self._calculate_sentiment(sent)
            gua = self._map_to_gua(sentiment)
            self.gua_results.append({
                "sentence": sent.text,
                "sentiment": sentiment,
                "gua": gua[0],
                "explanation": gua[1]
            })
    
    def _analyze_polarity_and_intensity(self) -> None:
        for result in self.gua_results:
            gua_name = result["gua"]
            # ç»Ÿè®¡ææ€§
            for polarity, guas in GUAS.items():
                if gua_name in guas:
                    self.polarity_stats[polarity] += 1
                    break
            # ç»Ÿè®¡å¼ºåº¦
            for intensity, guas in INTENSITY_RANK.items():
                if gua_name in guas:
                    self.intensity_stats[intensity] += 1
                    break

    def _calculate_sentiment(self, sent) -> float:
        """å¢å¼ºå‹æƒ…æ„Ÿè®¡ç®—"""
        try:
            # åˆå¹¶è¯æ€§æƒé‡ã€æƒ…æ„Ÿè¯åº“å’Œå¦å®šè¯å¤„ç†
            pos_weights = {"VERB": 0.3, "ADJ": 0.5, "NOUN": 0.2}
            score = 0.0
            negation = False
            sent_length = len(sent)
            
            if sent_length == 0:  # å¤„ç†ç©ºå¥å­
                return 0.0

            for i, token in enumerate(sent):
                # æƒ…æ„Ÿè¯åº“ä¼˜å…ˆ
                if token.text in self.SENTIMENT_LEXICON:
                    word_score = self.SENTIMENT_LEXICON[token.text]
                    # æ£€æŸ¥å‰ä¸‰ä¸ªè¯æ˜¯å¦æœ‰å¦å®šè¯
                    negation = any(w.text in self.NEGATION_WORDS 
                                for w in sent[max(0,i-3):i])
                    score += -word_score if negation else word_score
                elif token.pos_ in pos_weights:
                    score += pos_weights[token.pos_]
            
            # åŠ å…¥å¥å­é•¿åº¦è¡°å‡å› å­
            return math.tanh(score * math.log(sent_length + 1) / sent_length)
        except Exception as e:
            logging.error(f"æƒ…æ„Ÿè®¡ç®—å‡ºé”™ï¼š{e}")
            return 0.0

    def _map_to_gua(self, score: float) -> Tuple[str, str]:
        """ä¼˜åŒ–å¦è±¡æ˜ å°„é€»è¾‘"""
        try:
            # æŒ‰é¡ºåºæ£€æŸ¥åŒºé—´ï¼ˆä¿®æ­£å¤å¦é‡å¤é—®é¢˜ï¼‰
            intervals = [
                (0.8, 1.0, ("ä¹¾", "å¤©è¡Œå¥ï¼Œå›å­ä»¥è‡ªå¼ºä¸æ¯")),
                (0.6, 0.8, ("ç¦»", "æ˜ä¸¤ä½œï¼Œç¦»ï¼Œå¤§äººä»¥ç»§æ˜ç…§äºå››æ–¹")),
                (0.4, 0.6, ("å·½", "éšé£å·½ï¼Œå›å­ä»¥ç”³å‘½è¡Œäº‹")),
                (0.2, 0.4, ("è‰®", "å…¼å±±è‰®ï¼Œå›å­ä»¥æ€ä¸å‡ºå…¶ä½")),
                (0.0, 0.2, ("å¤", "åœ°åŠ¿å¤ï¼Œå›å­ä»¥åšå¾·è½½ç‰©")),
                (-0.2, 0.0, ("éœ‡", "æ´Šé›·éœ‡ï¼Œå›å­ä»¥ææƒ§ä¿®çœ")),
                (-0.4, -0.2, ("å…‘", "ä¸½æ³½å…‘ï¼Œå›å­ä»¥æœ‹å‹è®²ä¹ ")),
                (-0.6, -0.4, ("å", "æ°´æ´Šè‡³ï¼Œä¹ åï¼Œå›å­ä»¥å¸¸å¾·è¡Œ")),
                (-1.0, -0.6, ("å¤", "åå¤å…¶é“ï¼Œä¸ƒæ—¥æ¥å¤"))  # ä¿®æ­£å¤å¦ä¸ºå¤å¦
            ]
            
            for lower, upper, gua in intervals:
                if lower < score <= upper:
                    return gua
            return ("æœªæµ", "ç‰©ä¸å¯ç©·ä¹Ÿï¼Œæ•…å—ä¹‹ä»¥æœªæµç»ˆç„‰")
        except Exception as e:
            logging.error(f"å¦è±¡æ˜ å°„å‡ºé”™ï¼š{e}")
            return ("æœªæµ", "æ˜ å°„å‡ºé”™ï¼Œé»˜è®¤æœªæµ")

class ReportGenerator:
    def __init__(self, analyzer: YijingAnalyzer):
        self.analyzer = analyzer
        self.report = []
        
    def generate(self) -> List[str]:
        self._add_header()
        self._add_statistics()
        self._add_polarity_analysis()
        self._add_intensity_analysis()
        self._add_gua_analysis()
        self._add_detailed_mapping()
        return self.report
    
    def _add_polarity_analysis(self):
        """æ·»åŠ ææ€§åˆ†ææŠ¥å‘Š"""
        total = sum(self.analyzer.polarity_stats.values())
        if total == 0:
            return
            
        self.report.extend([
            "æ–‡æœ¬ææ€§åˆ†æ",
            "-" * 40,
            "ğŸ“Š ææ€§åˆ†å¸ƒï¼š"
        ])
        
        for polarity, count in self.analyzer.polarity_stats.items():
            percentage = count / total * 100
            polarity_cn = {"positive": "ç§¯æ", "neutral": "ä¸­æ€§", "negative": "æ¶ˆæ"}[polarity]
            self.report.append(f"  - {polarity_cn}ï¼š{count}æ¬¡ ({percentage:.1f}%)")
        self.report.append("")
    
    def _add_intensity_analysis(self):
        """æ·»åŠ å¼ºåº¦åˆ†ææŠ¥å‘Š"""
        total = sum(self.analyzer.intensity_stats.values())
        if total == 0:
            return
            
        self.report.extend([
            "æƒ…æ„Ÿå¼ºåº¦åˆ†æ",
            "-" * 40,
            "ğŸ“ˆ å¼ºåº¦åˆ†å¸ƒï¼š"
        ])
        
        for intensity, count in self.analyzer.intensity_stats.items():
            percentage = count / total * 100
            intensity_cn = {"high": "é«˜", "medium": "ä¸­", "low": "ä½"}[intensity]
            self.report.append(f"  - {intensity_cn}å¼ºåº¦ï¼š{count}æ¬¡ ({percentage:.1f}%)")
        self.report.append("")

    def _add_header(self):
        """æŠ¥å‘Šå¤´éƒ¨ä¿¡æ¯"""
        self.report.extend([
            "æ˜“ç»æ–‡æœ¬åˆ†ææŠ¥å‘Š",
            "=" * 40,
            f"ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"åˆ†ææ¨¡å‹ï¼š{DEFAULT_MODEL} (spaCy v{spacy.__version__})",
            ""
        ])
    
    def _add_statistics(self):
        """ç»Ÿè®¡ä¿¡æ¯æ¨¡å—"""
        stats = {
            "æ€»å¥å­æ•°": len(self.analyzer.sentences),
            "å¦è±¡åˆ†å¸ƒ": defaultdict(int)
        }
        for res in self.analyzer.gua_results:
            stats["å¦è±¡åˆ†å¸ƒ"][res["gua"]] += 1
        
        self.report.extend([
            "ç»Ÿè®¡æ‘˜è¦",
            "-" * 40,
            f"ğŸ“Š æ€»å¥å­æ•°ï¼š{stats['æ€»å¥å­æ•°']}",
            "ğŸ“ˆ å¦è±¡åˆ†å¸ƒï¼š"
        ])
        for gua, count in stats["å¦è±¡åˆ†å¸ƒ"].items():
            self.report.append(f"  - {gua}å¦ï¼š{count}æ¬¡ ({count/stats['æ€»å¥å­æ•°']:.1%})")
        self.report.append("")
    
    def _add_gua_analysis(self):
        """å¦è±¡æ·±åº¦è§£æ"""
        self.report.extend([
            "å¦è±¡ç‰¹å¾åˆ†æ",
            "-" * 40
        ])
        gua_explanations = {}
        gua_attributes = {}
        
        for res in self.analyzer.gua_results:
            gua = res["gua"]
            if gua not in gua_explanations:
                gua_explanations[gua] = res["explanation"]
                # è·å–å¦è±¡å±æ€§
                if gua in GUA_ATTRIBUTES:
                    gua_attributes[gua] = GUA_ATTRIBUTES[gua]
        
        for gua, exp in gua_explanations.items():
            self.report.extend([
                f"ã€{gua}å¦ã€‘è§£æï¼š",
                f"  å¦è¾ï¼š{exp}"
            ])
            
            # æ·»åŠ å¦è±¡å±æ€§ä¿¡æ¯
            if gua in gua_attributes:
                attrs = gua_attributes[gua]
                self.report.extend([
                    f"  äº”è¡Œï¼š{attrs['element']}",
                    f"  æ€§è´¨ï¼š{attrs['nature']}",
                    f"  æ–¹ä½ï¼š{attrs['direction']}"
                ])
            
            self.report.append("  å…¸å‹ä¾‹å¥ï¼š")
            examples = [r["sentence"] for r in self.analyzer.gua_results if r["gua"] == gua][:3]
            for ex in examples:
                self.report.append(f"  - {ex[:50]}...")
            self.report.append("")
    
    def _add_detailed_mapping(self):
        """è¯¦ç»†æ˜ å°„è¡¨"""
        self.report.extend([
            "å¥å­-å¦è±¡æ˜ å°„æ˜ç»†",
            "-" * 40,
            "åºå· | å¦è±¡ | æƒ…æ„Ÿå€¼ | å¥å­æ‘˜è¦"
        ])
        for idx, res in enumerate(self.analyzer.gua_results, 1):
            self.report.append(
                f"{idx:04d} | {res['gua']:2} | {res['sentiment']:+.2f} | "
                f"{res['sentence'][:60].replace('\n', ' ')}..."
            )

def main():
    """ä¸»æ§ç¨‹åº"""
    parser = argparse.ArgumentParser(description="æ˜“ç»æ–‡æœ¬åˆ†æç³»ç»Ÿ")
    parser.add_argument("-i", "--input", type=Path, required=True)
    parser.add_argument("-o", "--output", type=Path)
    args = parser.parse_args()

    analyzer = YijingAnalyzer()
    analyzer.analyze_text(args.input.read_text(encoding="utf-8"))
    
    report = ReportGenerator(analyzer).generate()
    
    # æ§åˆ¶å°è¾“å‡º
    print("\n".join(report[:50]))
    
    # æ–‡ä»¶è¾“å‡º
    if args.output:
        args.output.write_text("\n".join(report), encoding="utf-8")
        print(f"æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{args.output}")

if __name__ == "__main__":
    main()